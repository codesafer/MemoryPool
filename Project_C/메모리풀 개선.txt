/*----------------------------------------------------------------
	캐시라인 단위 정렬 기능 추가
----------------------------------------------------------------*/
캐시라인 단위 정렬 추가(64 배수 이상 단위 요청시 자동)
CMemoryPool_Manager::mFN_Calculate_UnitSize
에 의존적

Header내용	
	메모리풀에 직접 접근가능한 포인터 / 또는 메모리풀 관리자 포인터

※ 주의
	풀 내에서 n개의 메모리 유닛 단위로 할당할때
	기본적으로 캐시라인에 맞춰 할당


※ 주의
	만약 사용자가 만든 구조가 캐시라인 크기에 정확히 맞춰만든 구조(헤더크기를 고려하지 않은)라면
	헤더크기가 더해져 캐시라인크기*2배가 할당이되는 참사가 발생한다
----------------------------------------------------------------
풀크기 종류 개선(헤더 크기 포함)
	CMemoryPool_Manager::mFN_Calculate_UnitSize
		다음 설정에 의존
		gc_array_limit[]		구간
		gc_array_MinUnit[]		배수(최소단위)



/*----------------------------------------------------------------
	추가 개선A
----------------------------------------------------------------*/
(완)
1. CMemoryPool_Linker::TLink::GetNext() 수정
	TLink 동적 할당부분을 락 프리하도록 적용

(완)
2. 메모리풀 링커 수정(Find Pool 부분)
	읽기는 락 프리하도록 수정
	싱글톤의 created flag 방식 응용
	creating flag
	TLink 들을 수회하여 풀을 찾지 못하는 경우 이 플래그를 확인
	플래그를 소유한 스레드는 그사이 다른 스레드가 껴든 경우를 체크,
	TLink들을 한번더 순회(두번째 확인)
		mFN_Link_MemoryPool 의 순서는,
		pLink->m_pPools
		pLink->m_UnitSizes

	-> ※ mFN_Find_pool , mFN_Link_MemoryPool 인라인화 할 경우 성능하락

	결과: 다중 스레드에서 실행시 mFN_Link_MemoryPool 점유율 50% 이상 -> 개선후 3% 미만

(완)
3. 사용자 혼란 문제
메모리 할당, 해제시 MemoryPoolResource 를 상속받지 않고 사용시 다음의 문제가 있다
캐시라인 정렬 문제
	MemoryPoolResource 을 상속시, 사용자는 추가되는 헤더가 사용자 객체에 포함되는 것을 인식하기 때문에,
	캐시라인 정렬에 문제가 없다
하지만 상속을 하지 않고 메모리 할당을 요청시
사용자가 만약 캐시라인 정렬된 메모리를 원한다면 문제가 된다
메모리 할당단위가, 다음과 같이 헤더를 포함하기 때문
[ 캐시라인 단위]
[헤더][   data  ]

	이는 두가지 문제가 있다
	1. 메모리 낭비 문제
		사용자타입이 캐시라인에 정확히 맞춘 64바이트라면 헤더가 포함되면서 128이 된다
	2. 사용자 데이터의 캐시라인 정렬이 어긋나는 문제
		사용자가 사용하게 되는 주소는 data 부분이다
		실제 캐시라인 단위로 시작되는 곳은 헤더 부분이기 때문에
		사용자가 의도하는 객체의 시작부분, 객체의 중간 부분등의
		캐시라인 정렬이 되지 않는다

	해결할 방법
	Memory Get / Return 메소드들은 두가지 타입이 있다
	일반 , Header
	이중 Header 형식은 MemoryPoolResource 상속형태로 사용한다
	일반 Get / Return 의 방식을 CacheAlign 방식 메소드를 각각추가
	[헤더64][data] 형식으로 사용하도록 한다


	cachealign 형식의 경우 헤더주소 접근을 특별하게 처리한다
		header = ptr - 64B
	또한 pool의 타입은 헤더64, 데이터64 포함 최소 128B 이상부터 사용되어야 하며오후 8:05 2015-07-10
	이를 어길시 에러를 발생시켜야 한다

	추가 적으로 PoolManager는 Pool을 리턴할때, force Cachealign 옵션을 받도록 하여
	force cachealign 옵션이 true 라면,
	(64 + 원하는 사이즈를 포함하는 64의 배수) 크기의 풀을 리턴하도록 해야 한다
	※ 이때 mFN_Calculate_UnitSize 는 무시한다
	결과적으로 64B 낭비문제는 해결하지 못한다

(완)
4. mFN_Default_NEW 가 사용될시 경고를 해줘야 한다
	디버깅 모드에 한하여 간단하게 경고


(완)
5 메모리풀의 메모리 보관 방식 개선
	(메모리 절약, basket의 불필요한 메모리 할당, 메모리 단편화 방지)
	기존 스택에 보관
	개선목표
	풀에서 보관시 객체의 헤더에 다른 객체의 포인터를 링크
	FIFO 방식
	[next][] .... [next][]
	Get 요청시 풀의 포인터를 기록
	Return(반납) 시 next 포인터 기록

	사용 메모리 확인 비교

	수정전
	타입 크기 128, 512, 4096
	1GB	228K
	100MB	23K

			1 * 228K	T1	10 * 228K T1	1 * 23K T1	1 * 23K T4	1 * 23K T8
	기본t		0.6710		6.6300		0.0940		0.3280		0.7020
	기본MB		1050.715		1055.008		109.867		422.688		814.047

	풀t		0.4530		1.4660		0.0470		0.2340		0.9040
	풀MB		1119.727		1119.297		119.234		453.055		885.301

	풀t(개선)		0.4530		1.9190		0.0620		0.2340		0.6000
	풀MB(개선)	1112.676		1112.672		118.461		441.648		799.504
	기존대비		100%		131%		132%		100%		66%

	개선 요약
		싱글스레드 성능 하락
		다중스레드 성능 상승
		사용 메모리 매우 감소(유닛 크기가 작고 할당수가 많을 수록 더 이익)

	기본할당자와 비교
		반복 회수가 많을수록 유리함
		사용가능한 유닛수(할당수)가 더 많음
		작은 유닛단위에서 상대적으로 느림
		큰 유닛단위에서 속도는 1~8 스레드 모두 유리함(평균 512)

/*----------------------------------------------------------------
	추가 개선B
----------------------------------------------------------------*/
1. [추가 개선A]의 오류 수정
2. 기존 GetCMD, ReturnCMD 에서 각각 모든 스레드가 공유하는 인덱스 사용,
	이로 인해 두 함수의 점유율을 합쳐 70%
	이 구조로는 윈7 LFH 를 따라 잡을 수 없음

	-> GetCurrentProcessorNumbe 함수를 이용(현재 스레드가 실행중인 프로세서 인덱스 리턴)
	프로세스 수만큼 만들어진 Basket에 즉시 접근
	이때 문제는 실행 중간에 현재 스레드가 실행중인 프로세서가 변경될 수 있다는 것
	(단순히 더하기 빼기 반복적인 일을 해도)
	이로 인해 Lock을 사용할 수 밖에 없음

	※ 개선 구조
		기존 프로세서수/2개 Basket 을 프로세서수만큼 사용
		현재 스레드의 프로세서번호와 1:1 매칭
		스레드가 동작하는 프로세서가 임의로 변경되는 문제로 Lock은 사용해야 함
		(SetThreadAffinityMask 을 사용해도 소용 없음)
		
		기존 메모리 확장 요청 처리
			할당자(실제 할당) -> Basket 들에 균등 분배
			※ 반납시 Basket 들에 균등 반납
		개선 메모리 확장 요청 처리
			할당자(실제 할당) -> 메모리풀에 보관
			개별 Basket 은 필요시 메모리풀에서 n개를 가져감
			※ 반납시 현재/기존 Basket에 반납

	Loop 1 , 128B * 10000K, T1 테스트에서,			1.8 Sec
		mFN_Get_Baset()	14.4%	86C
		mb.Lock()	8.4%	50C
	Loop 1, 128B * 10000K, T4(스레드당 2500K) 테스트에서		0.9 Sec
		mFN_Get_Baset()	10.7%	95C(스레드당 23.75C)
		mb.Lock()	14.8%	131C(스레드당 32.75C)

		다중 스레드에서 오버헤드가 적음
		스레드간 간섭이 적다는 것을 증명


	■성능 테스트 B-1
	Test 128B * 10000K(시간은 1회당 평균)	CPU 물리코어8
			T1		T2		T4		T8		T16
	1 * LFH		1.7166(1455MB)	1.2374(1455MB)	1.2193(1439MB)	1.2615(1383MB)	1.0751(1088MB)
	1 * 메모리풀	1.9172(1383MB)	1.2553(1382MB)	0.8997(1306MB)	0.8263(1215MB)	1.1389(641MB)
	10 * LFH		1.6807(1461MB)	1.2473(1458MB)	1.2362(1463MB)	1.2834(1452MB)	1.2038(1366MB)
	10 * 메모리풀	1.5216(1863MB)	0.8019(1969MB)	0.4785(1845MB)	0.3566(1523MB)	0.3982(1273MB)

	스레드가 많을 수록, 반복횟수가 많을 수록 더 빠르다(물리적 코어수 이하의 스레드에서)
	반복횟수가 많을 수록 메모리할당을 초과 요청할 가능성이 높다(이는 이론적으로 최악의 경우 필요메모리*프로세서수)
		-> 이는 특정 basket으로부터 get / return 을 요청할때 현제 스레드의 프로세서번호(basket 인덱스로 직결)가 바뀌는 현상에서 비롯된다
		    현재 basket::의 balance 를 이용한 메모리풀에 반환하는 로직을 개선하거나,
		    basket과 1:1 매칭 되는 중간 완충지역이 필요하다(CACHE 형태)



	■성능 테스트 B-2(Basket 에 반납용 CACHE 추가)
	Test 128B * 10000K(시간은 1회당 평균)	CPU 물리코어8
			T1		T2		T4		T8		T16
	1 * LFH	
	1 * 메모리풀
	10 * LFH	
	10 * 메모리풀							0.4029(1360MB)
	100* 메모리풀							0.3636(1369MB)
	프로파일링후 설정 수정

	CACHE들에 있는 데이터를 재활용함으로써 실제 할당 메모리를 LFH 이하로 감소

	발견된 문제점
		x86 모드에서 성능의 급격한 저하
			GetCurrentProcessorNumber	점유율 56.5%
		x64
			GetCurrentProcessorNumber	점유율 32.4%

		프로세서 번호를 구하는 빠른 방법이 필요	

		※ 테스트를 위해 다음 함수를 비 인라인화중
			TMemoryBasket& CMemoryPool::mFN_Get_Basket()

		수정 방법 1 GetCurrentProcessorNumberVerX86 를 사용시 점유율이 기존 대비 절반으로 줄어드나, 
			    GetCurrentProcessorNumber 보다 느림

			GetCurrentProcessorNumber 보다 빠른 함수는 ?

			sidt 라는 함수
				x86 50 : 0.1   x64 30 : 0.2

		적용후 x86 x64 모두 30% 이상 성능 개선

	■성능 테스트 B-3(프로세서 번호 구하는데 sidt 사용)
	Test 128B * 10000K(시간은 1회당 평균)	CPU 물리코어8
			T1		T2		T4		T8		T16
	1 * LFH		1.7166(1455MB)	1.2374(1455MB)	1.2193(1439MB)	1.2615(1383MB)	1.0751(1088MB)
	1 * 메모리풀	1.5030(1382MB)	1.1298(1382MB)	0.87841306MB)	0.8031(1215MB)	1.5782(641MB)
	10 * LFH		1.6807(1461MB)	1.2473(1458MB)	1.2362(1463MB)	1.2834(1452MB)	1.2038(1366MB)
	10 * 메모리풀	1.1206(1385MB)	0.6223(1391MB)	0.3821(1385MB)	0.3804(1344MB)	0.4855(640MB)

	★★★ 기존 GetCurrentProcessorNumber 와 비교할때 sidt 사용은 x86 과 x64 간의 성능차이를 거의 없게 바꿨다

	■SIDT TLS 캐시 적용
		적용 방식
			DLL TLS는 보통 TlsAloc ~ TlsFree TlsSetValue ~ TlsGetValue 를 사용해 구현하는데
			이를 DLL을 사용하는 라이브러리 또는 응용프로그램에서 static TLS를 전달하는 함수포인터를 DLL에 전달
			더 간단하게 처리
			

		결과 샘플 : 10 * 10000K T8
			CacheMiss 부분에서 보다시피 GetBasket의 호출(2 * 10 * 10000K) 에서 프로세서가 바뀌어 다시 계산하는 빈도는 매우 적다
		Pool[136] = n[   5985178] / n[   9649342], call alloc[       815]
		0 = n[      7710] nDemand[      7710] Get[   1670788] Ret[      4830] CacheMiss[        19]
		1 = n[      7710] nDemand[      7710] Get[   1206458] Ret[       538] CacheMiss[        17]
		2 = n[      7710] nDemand[      7710] Get[   2215455] Ret[         1] CacheMiss[        12]
		3 = n[      7710] nDemand[      7710] Get[   1534145] Ret[      3550] CacheMiss[        13]
		4 = n[      7710] nDemand[      7710] Get[   1679663] Ret[      6647] CacheMiss[        20]
		5 = n[         0] nDemand[        30] Get[         0] Ret[         0] CacheMiss[         0]
		6 = n[         0] nDemand[        30] Get[         0] Ret[         0] CacheMiss[         0]
		7 = n[      7710] nDemand[      7710] Get[   1693491] Ret[       388] CacheMiss[        27]

	Test 128B * 10000K(시간은 1회당 평균)	CPU 물리코어8
	테스트는 x64
	(x86 vs x64 성능차 :  약 x86 0.8818 vs x64 0.8688)
				T1		T2		T4		T8		T16
	1 * SIDT			1.3973						0.8688
	1 * SIDT + TLS 캐시	1.4280						0.8121
	10 * SIDT		1.0950		0.6075		0.3816		0.3628		0.4668
	10 * SIDT + TLS 캐시	1.0879		0.6065		0.3882		0.3655		0.4333

	30 * 16777216(2GB) T8 테스트
	SIDT			0.5413
	SIDT + TLS 캐시		0.5448

	정리
		SIDT(TLS 캐시 미사용 버전)는 8 프로세서 기준에서는 순회해야 하는 수가 8정도로 작기 때문에 성능 향상에 별 차이가 없다
		만약 프로세서 수가 더 적다면 TLS 캐시는 스레드 TLS 접근비용이 오버헤드가 되어 오히려 성능이 감소할 것으로 예상된다
		(하지만 이것은 무시해도 될 정도로 미미할 것으로 본다)

	결론
		SIDT + TLS 캐시버전은 프로세서수가 매우 많아지는 (서버용) CPU에서의 성능감소를 대비하기 위함이다


	★차후 가능한 개선
	1. allocator 은 더 개선이가능한가?
	2. Baskets 간 CACHE를 활용한 유닛 재활용은 아직 설정이 비성숙단계
	   충분히 더 최적화 가능

	■Initialize 하지 않고 mFN_Add_Reserve을 호출할 경우 자동Initialize에서 크기 계산 오류를 수정

	■할당자의 병목현상
		이 문제는 더 개선이 가능한가?
		객체크기 128B	반복1 * 10000K 에서 MemoryPoool 내에 Alloc 의 소모시간 비율
		T1	28.5%
		T8	70.5%
/*----------------------------------------------------------------
	추가 개선C
		vs TBBMalloc
		작은 사이즈 128B 이하에서는 TBB가 유리
----------------------------------------------------------------*/
오브젝트 풀 관련...

	(완)
	오브젝트 풀 추가 CObjectPool
	다음의 핸들러를 이용해 컨트롤(동적단위 크기 사용 불가능)
	CObjectPool_Handle
	CObjectPool_Handle_TSize

	(완)
	CMemoryPool 과 CObjectPool 모두 싱글톤이기 때문에
	생성 삭제순서에 문제가 생긴다
		CMemoryPool 보다 CObjectPool 이 나중에 생성되고 삭제는 CObjectPool 이 먼저 삭제 되지만
		~CMemoryPool 에서 필요로 하는 경우
	이를 위해 삭제시기 연장이 가능한 싱글톤 추가
	CSingleton_ManagedDeleteTiming

	이 오브젝트 풀을 적당한 곳에 적용
		(완)	TDATA_VMEM_USED
		(완)	_SAFE_NEW_ALIGNED_CACHELINE(pReturn, CMemoryPool(UnitSize))
		(완)	_SAFE_NEW_ARRAY_ALIGNED_CACHELINE(m_pBaskets, TMemoryBasket(m_nUnit_per_Page), m_nBaskets)
				m_pBaskets 의 경우 실행환경 따라 동적 크기 인데, 해당 오브젝트 풀을 비교하여 연결하여 사용할 것
		(완)	CMemoryPool_Linker::TLinkPool_Chunk


	(완)Virtual Alloc 부분, reserve commit 분할된 것을 합치도록..

	(완)
	MemoryPool_Interface.h
	이 파일은 사용자에게 제공되기 때문에(보안이 없음)
	구현핵심 부분을 짐작할만한 키워드를 다른 이름으로 수정해야함
	매크로, 객체이름등

	(완)
	mb 에서 다른 mb를 잠금을 시도할때
	데드락의 가능성이 있다
	spinlock 에 스핀제한후 리턴 기능을 추가해서 사용해야 한다

	(완) CSpinLock 에 Lock_Busy 를 추가 Test 없이 바로 Set 하도록 하여, 스레드 동시 진입률이 낮은 곳에 사용
/*----------------------------------------------------------------
	추가 개선D
		캐시 밸런싱
----------------------------------------------------------------*/
(완료된 것들)
	MSDN 샘플 코드를 사용하여 프로세서수를 논리적 / 물리적 구분 하여 메모리풀에 활용

	사용자용 디버깅 코드 추가
	MemoryBasket 객체내의 캐시부분을 별도로 분할
	기존 Basket : cache
	1 : 1 관계에서
	n : 1 관계로 수정
	cpu에 따라 n개의 Basket이 한개의 cache를 공유
	basket , cache 의 수요 밸런스 수정
	-> basket 로컬저장소(프로세서 기준)의 수요를 오브젝트의 크기가 큰경우 0이 가능하도록설정
	   최대 로컬저장소 크기 : 64KB / OBJ_Size / 논리적코어수
	   (이로 인해 다른 프로세서의 로컬저장소 공간에 데이터가 사용하지 못하고 낭비되는 크기를 줄임)
	   CACHE(keep / share) 또한 최대크기를 적용, 좀더 여유로운 크기를 적용

	mFN_Get_MemoryHead_Process 의 로직을 더 효율적으로 개선
		수요 증감 계산 개선
		메모리를 얻어오는 과정을 더 고속으로

		동적할당부분 사용부분 최대한 제거

	-> 낭비되는 메모리 감소, 속도 상승

	통계 기능 추가
		IMemoryPool
		TMemoryPool_Stats::mFN_Query_Stats

	질의 기능 추가		
/*----------------------------------------------------------------
	추가 개선E ★★★
		alloc free 절차 개선
		속도, 사용 메모리 대폭 개선
----------------------------------------------------------------*/
작은 유닛의 경우 유닛별로 포함된 헤더 때문에 메모리 낭비가 심함
작은 유닛의 경우 TBB에 비하여 속도가 10% 이상 느림

Alloc 부분에서...
	if(mb 보유수가 소요보다 작을때)
		반납 -> mb
	위코드는 mb가 가득차면 cache에 접근하여 한번에 1개를 반납하게 되는데,
	만약 mb : cache가 1:1이 아니라 2:1 이상(공유되는 캐시)의 경우 cache의 접근은 병목지점이 된다
	또한 그렇지 않더라도 mb, cache 모두 잠금을 하기 때문에 비용이 크다
	개선책 : mb가 가득차면 cache로 모두 옮긴다
	이 처리를 한다면 다중 mb에서 각각에 고립되는 데이터를 줄일수 있다
	(완)

다음의 패턴으로 사용한다면...
	for(;;) new
	for(;;) delete
	popFront / pushFront 시 메모리순서가 아래와 같이 섞인다
	0 1 2 3 4 5 -> 3 2 1 0 5	
	popFront , pushBack 형태로 하도록한다
	※ 먼저 나간것이 먼저 회수될 확률이 높다고 예상한다
	(완)

	

이하 완료된 것들....
	헤더부분 개선
		유닛 사이즈를 3그룹 나눠 작은/일반/큰(예외형)
		작은 유닛은 64KB에서 앞의 64B만 사용, 다른 경우는 유닛마다 헤더 64 포함
		이것은 상당한 메모리 절약을 가능하게 했다(작은 유닛의 헤더방식은 TBB의 방식을 참고)
		(하나의 64KB그룹내의 작은 유닛들은 하나의 헤더를 공유)
		그리고 이것은 삭제시 검증을 목적으로 전역 헤더 링크 테이블을 둔다
	메모리 헤더 유효성 확인 전역 테이블
		테이블의 최대 링크 수의 제한은 주소 크기를 기준으로 하여 x64(67,108,864), x86(268,435,456)
		이때 테이블의 용량 차지는 x86x64 공통 511.9375MB(초기치는 64KB)
		이때 x64에서 가장 작은 단위8로 테이블을 가득 채울시(그것도 최악의 상황인 64KB를 한블록 단위로만 할당할때)
		헤더를 포함하여 4TB 메모리까지 링크 가능(이때 유닛은 549,218,942,976)
		작은유닛 기준중 가장 큰 2048을 기준으로 하면, 사용하지 못하는 공간이 생겨 이 경우 64KB당 31유닛으로 계산
		이때 유닛은 ( 2,080,374,784 ) 사용자가 사용하는 크기는 3968 GB
		만약 확장되는 블록의 크기제한을 크게 잡는다면 사용가능한 크기는 더 올라감
		x86에서 테이블의 크기로 인한 관리가능한 수의 제한은 없음(x86의 사용메모리 제한으로 인하여)

	전역 메모리풀 접근을 위한 색인 테이블에 모든 메모리풀이 등록되도록 수정
		식을 수정하여 테이블의 크기또한 대폭 감소

	예외형 메모리풀 추가
		기존 취급하지 않는 크기를 메모리풀 관리자에서 처리했는데 예외형 메모리풀에서 처리하도록 하고
		통계기록

	디버깅모드시 속도가 느린 부분(대표적으로 leak 추적) ON/OFF 기능 추가
		mFN_Set_OnOff_Trace_MemoryLeak
		mFN_Set_OnOff_ReportOutputDebugString

	메모리 반납시 유효성 확인
		디버깅시에는 헤더 손상까지 모두 확인

	interface 정리

	캐시밸런싱 수정
		유닛의 mb, cache, pool 간 이동 개선

	속도개선 가장 비중이 높은...
		SpinLock의 UnLock부분 개선(기존 로직 프로파일링 결과 1스레드에서 Lock 과 UnLock 이 비슷한 시간을 소모하고 있었음)

요약
	메모리 절약, 반납 메모리 검사(crush 방지) 부분으로 속도가 10%이상 하락 하였고
	스핀락 개선으로 그 이상 속도가 상승함
/*----------------------------------------------------------------
	오류 수정 : 추가 개선E
----------------------------------------------------------------*/
심각한 오류(메모리 과다 할당)
	재현 어려움
	Basket의 수요를 초과하여 반납되는 현상
	-> mFN_Return_Memory_Process_Default 에서 Basket에서 cache로 이동시킬때 일정수 이상만 이동시키기 위한 조건이 잘못되어 대용량의 메모리가 basket에 고립되는 현상
	if(mb.m_nDemand < GLOBAL::gc_LimitMinCounting_List)
	유닛수가 아닌 수요와 비교한게 잘못
/*----------------------------------------------------------------
	완료
----------------------------------------------------------------*/
STL Container allocator 호환

mFN_Return_MemoryQ 추가
	헤더 손상, 반납주소 유효성 체크를 하지 않거나 최소한으로 함
	성능향상은 14%
/*----------------------------------------------------------------
	앞으로 계획
----------------------------------------------------------------*/
메모리 유닛의 보유에 있어서...
	기존 링크드리스트 형태는
	n개씩 다른 다른 레벨의 캐시로 이동할때 CPU 캐시미스율이 높다
	이것을 해결하기 위한...
	1) 유닛들을 주소순서대로 정렬
		A) 정렬기능 추가
		B) 리스트에 추가시 주소순으로 삽입
	2) 포인터들을 그룹화(64B단위)
		struct ??{
			void* p[64 / sizeof(void*)];
		};
		문제는 추가 메모리 사용
		-> 유닛크기 단위 8B 의 경우 매우 심각함
		-> 
	만약 이것을,
	64개 
	

헤더에 체크섬을 추가할 것인가?



실행도중 해제 기능(OS 에 반납하는)
만약 이 기능을 만든다면, 기존 메모리풀이 아니라
새롭게 다른 버전으로 만드는 편이 낫다
	헤더의 정보에는 자신이 할당된 정보를 가진 TDATA_VMEM_USED 에 연결할수 있어야 하며
	해제 요청이 가능해야한다	VirtualFree
	ParamNormal을 이용
		1. 연결에 필요한 정보
			CMemoryPool_Allocator_Virtual
			-> 메모리풀 주소가 있으니 불필요하다
			풀을 통해 접근 가능함 void** 가 필요함
			m_Array_pUsedPTR[n] 에 자신이 쓰여진 위치
		2. 사용 카운팅(다중 스레드에서 문제가 될지 모른다)
			atomic 하게 체크해야 한다
	해제를 위해서는 추가적인 작업이 필요한데
	메모리풀에서 이 그룹의 데이터(UNIT)들을 분리하여 제거해야한다
	또는 구조적으로 데이터 그룹단위로 메모리풀내에서 보관하는 방법도...
	OS에 해제하는 작업은 자동으로 아니면 사용자에 의해 정리하는 매소드 호출로?
	이 작업은 쉽지 않다
	/*----------------------------------------------------------------
	->
	기존의 3단계 캐시단계를 수정해야 할지도 모른다
	반납의 경우를 수정, 각 메모리별 VirtualAlloc된 그룹단위로 TDATA_VMEM_USED 에 반납을 받는다면?
	기존 구조에서 TDATA_VMEM_USED는 n개가 아닌 1개 할당단위 구조로 변경해야 한다
	struct _DEF_CACHE_ALIGN TDATA_VMEM_USED{
		// 필수 데이터(양방향 링크 : 중간에 위치한 노드의 삭제를 위해)
		TDATA_VMEM_USED* pPrev
		TDATA_VMEM_USED* pNext;
		CSpinLockQ m_Lock; // 서로 다른 프로세서간 간섭
			데드락 주의
				현재 노드가 삭제 될때 앞/뒤 노드를 서로 연결해줘야 할때...
				기준 노드로부터 현재 노드, 앞 뒤 노드 모두 잠금
				이때 앞이나 뒤 노드 또한 같은 작업을 하려 한다면?
				-> Lock__NoInfinite 사용,
				-> 또 다른 문제로 조금 복잡해 진다
				공용 1개 잠금을 사용하면 간단하지만 경쟁이 너무 심해진다

		TMemoryObject* pFirst;
		TMemoryObject* pLast;
		size_t nUnits_Keep;
		size_t nUnits_Allocated;

		size_t _free_slot; // 여유슬롯 8B
	};
	-> 반납시 문제점으로 스레드간 경쟁이 발생할 수 있다
		하나의 VMEM 을 여러 스레드가 동시접근이 많은 경우
		아주 작은 양의 메모리를 계속 할당/해제 반복하는 경우 경쟁 가능성이 있다
		-> 기존 mb 처럼 프로세서수만큼 또는 절반만큼의 식으로 하나의 VMEM을 여러개로 분할,
		   사용하여 경쟁을 회피하는 가능하지만 메모리 비용이 너무 커진다
		   이 경쟁 회피도 결국 F, L 유닛삽입 nUnits 카운팅 까지만 적용된다. 최종 카운팅 합계는 결국 프로세서간 간섭

	■ 모두 반납 + 임의추가조건 만족시
		VMEM 리스트에서 제거
		VirtualFree 실행
		g_Table_BlockHeaders_Normal 에서 Unregister
		※ 이때 잠금에 매우 각별한 주의가 필요하다(인접 노드또한 잠금을 시도할 수 있으며) AB 문제에 주의

	■ 성능을 위해 추가 가능한 옵션
		메모리풀 별 일정 크기 까지는 OS에 반납하지 않고 보유하는 옵션
		(사용자의 수요예측에 의한 설정이 가능하도록)

	■ 설정
		L1 프로세서별 전용 공간

		POOL[1]
			Allocator[1]
			VMEM(list) : instance

			VMEM(링크 그룹들)
			struct TDATA_VMEM_LINK{
				TDATA_VMEM_LINK* pNext
				size_t cnt;
				TDATA_VMEM_USED* pLink[x86=12, x86=6]
			};
			▶ TDATA_VMEM_LINK* 사용가능(유닛이 남아있는 VMEM들)
			▶ TDATA_VMEM_LINK* 가득찬(유닛이 가득차 필요시 삭제)

	▶ 할당 시나리오
		▷ L1 데이터 확인
			데이터는 기존 F, L, N 을 고수?
			아니면 TDATA_VMEM_USED 를 링크해서 전용으로 사용
				여기서 좀더 수정을 가하면
				1개의 TDATA_VMEM_USED를 여러 조각으로 분할하여 사용하는 것도 가능

	▶ 해제 시나리오
		▷ L1 에 반납
		if(L1 보유수 < L1 수요)
			return;
		▷ L1의 유닛들을 각 소속 VMEM에 반납
			이 경우의 문제는 단순히 유닛의 값만 읽는 것이 아니라
			유닛이 가르키는 주소에 반납하기 때문에 캐시 미스 확률이 높다
			-> 그렇다면 애당초 VMEM에 반납해야 하는가?
				하지만 결국 유닛단위로 VMEM에 반납하더라도,
				1. 유닛을 CPU 캐시에( 이경우는 사용자가 소멸자과정을 통해 이미 캐시에 있을 확률이 매우 높다)
				2. 유닛의 헤더를 CPU 캐시에(이것은 어쩔수 없다)
				3. 헤더가 가르키는 VMEM에 접근(이부분이 캐시미스 확률이 있다)
				[3]의 경우는 L1에 유닛들을 모아 처리하면 같은 VMEM 소속이 여러개라면 캐시히트 이익
				-> 유닛의 주소는 당연히 읽고 쓰기때문에 CPU 캐시에 로드된다
				   문제는 유닛의 소속VMEM에 접근하기 위해 헤더를 접근하기 때문에 캐시미스 가능성이 있다(2중문제)
					-> 유닛의 데이터 공간에 VMEM을 링크해둔다면?
						이때 문제는 list 형태로 유닛들을 보유하기 때문에,
						유닛크기 8의 경우 포인터하나 크기 밖에 들어가지 않는다
						그러므로 유닛의 데이터 공간에 VMEM을 기록할 수 없다
						(유닛의 크기가 16을 넘으면 가능하게 분기할 것인가?)
	▶ 예상되는 문제점들
		할당요청시 L1 , L1에 연결된 VMEM 형태로 접근하면 잠금을 기본 2개를 사용하게 된다
		이때 VMEM 이 다른 프로세서와 동시에 사용하려 한다면 접근 비용은 매우 비싸다

		L1에 기존 방식대로 유닛들을 가져다 보유해두고 사용한다면?
		반납하는 다른 스레드(프로세서)와 경쟁을 줄일 수 있다

	▶ VMEM에 반납시 다른 스레드가 같은 VMEM에 접근(반납/할당요청으로 인한 취득)을 하는 경우 경쟁이 심각하다

	▶ 주요한 포인트는 각 VMEM를 하나의 프로세서에서 사용하도록 하는 것을 지향 하는 것
		만약 스레드의 현재 프로세서가 계속 바뀌더라도, 별로 문제가 되지 않는다
		프로세서들이 바쁘다면 일단 잘 안바뀐다. 바뀌더라도 반납시 캐시미스 불이익이 있을뿐이다

	/*----------------------------------------------------------------


코드내의 텍스트들
	디버깅용 그리고 릴리즈용
	만약 누가 디어셈블리할때 텍스트가 있다면 해석이 쉬울것이다

메모리풀 매크로 추가
	_macro_new_from_memorypool
	array 계열

/*----------------------------------------------------------------
	작업중
----------------------------------------------------------------*/
메모리풀 리팩토링...실행도중 VirtualFree가 가능하도록...
이 계획은 단순하다

가장 큰 문제는 기존 코드는 어떻게 하는가?
	방안1
		기존의 객체를 수정, 덮어 씌우기
	방안2
		새로운 객체에 기존의 이름을 사용
		기존의 객체는 오브젝트 풀로 변경
			(VirtualFree 하지 않는)
			오브젝트풀은 a~b 크기간 공유할 것인가? 또는 각 크기간 독립적으로 사용하는가?
	방안3
		방안2에 추가되는 방식으로
		오브젝트풀에 접근하는 방식
		사용자는 관리자에 오브젝트풀을 생성 요청하여 사용한다

기존 객체 에서...
	TDATA_VMEM_USED	기존 VirtualAlloc 단위......64B
		LOCK
		TMemoryUnitsGroup* pUnitsGroup[2] // 유닛 그룹은 2개 (최소 할당 단위는 64KB * 2 = 128KB)
		유닛들이 모두 반납되면 VirtualFree 실행



	TMemoryUnitsGroup	64B
		enum struct EState : UINT32{
			E_Full = 0,
			E_Rest = 1,
			E_Busy = 2,
			E_Empty = 3
		};
		TDATA_VMEM_USED* pOwner
		size_t nUnitsAllocated

		size_t nUnits
		TMemoryObject* pF
		TMemoryObject* pL
		LOCK m_Lock

		// 이하 남은 공간 16B
		EState m_State;
		

		TMemoryUnitsGroup 는 CACHE 에서 링크해서 사용한다.
		이때, 작은 유닛 단위라면 CACHE 와 1:1 관계(독점)
		큰 유닛단위라면 메모리 절약을 위해 공유해 사용한다 TMemoryUnitsGroup 1 : N CACHE
			공유와 관련하여 별다른 처리는 거치지 않는다
			단지, 메모리풀에 pList_Full 또는 pList_Rest가 없다면 pList_Busy도 가져다 사용할 뿐이다
			(작은 유닛에서 재활용이 상당히 나빠지는 문제가 있다)


	~TMemoryUnitsGroup



	TDATA_BLOCK_HEADER
		// 기존 FreeSlot에 다음 데이터를 추가
		// 반납시 접근 한다
		TMemoryUnitsGroup* pOwner

	Pool 은 TMemoryUnitsGroup 을 리스트 단위로 관리(상태가 변경된다면 위치를 바꾼다)
		가능하다면 lockfree linked list로 처리하고 싶다
	이것들의 First 노드가 변경되는 비용은 매우 비싸다....(Pool자체에 연결되어 있기 때문에...)
	TMemoryUnitsGroup* pList_Full		// 유닛이 가득찬
	TMemoryUnitsGroup* pList_Rest		// 사용된 + 연결안된
	TMemoryUnitsGroup* pList_Busy		// cache에 연결된
	TMemoryUnitsGroup* pList_Empty		// 모두 사용된
		위 데이터들을 각각 64B 단위로 분리 하고 싶지만(가짜 공유를 피하기 위해) 용량이 256 Byte필요하게 된다
	위 데이터는 다시 다음과 같이 수정 가능하다
	struct DECLSPEC_CACHEALIGN TMemoryUnitsGroupList{
		LockQ m_Lock
		TMemoryUnitsGroup*	m_pFirst
		TMemoryUnitsGroup*	m_pLast
		size_t 			m_CNT
		// 이하 남은 공간 32B
		// 통계로 활용?
	}
	// 자주 변경되는 것을 앞에?
	TMemoryUnitsGroupList	m_List_UnitsGroup_Busy
	TMemoryUnitsGroupList	m_List_UnitsGroup_Rest
	TMemoryUnitsGroupList	m_List_UnitsGroup_Empty
	TMemoryUnitsGroupList	m_List_UnitsGroup_Full

	TMemoryUnitsGroup 의 남은 수량 상태, CACHE에 연결됨 상태에 따라, 소속 위치는 변경되어야 한다




할당 프로세스 변경의 문제
	기존 CACHE L1 잠금 한단계 에서
	CACHE L1 잠금, 연결된 TMemoryUnitsGroup 잠금 으로
	두 단계 잠금을 사용한다는 문제가 있다

반납 프로세스 변경
	기존
		1. 메모리 헤더 접근					- miss
		2. 전역 헤더 테이블 접근				- miss
		3. CACHE 접근(L1, 옵션으로 최대 L2, L3 까지 접근)	- hit	★잠금
			4. Unit 간 연결					- miss
	새로운 구조
		1. 메모리 헤더 접근					- miss
		2. 전역 헤더 테이블 접근				- miss
		3. TMemoryUnitsGroup 접근				- miss	★잠금
			4. Unit 간 연결					- miss
		// 이하 옵션
		5. TMemoryUnitsGroup 의 상태 변경, Pool 에서 Group 위치 이동 - high cost
		6. 같은 소속(TDATA_VMEM_USED)의 TMemoryUnitsGroup 들이 모두 Full 이라면...	- high cost
			추가로 어떤 조건을 만족한다면 VirtualFree 또는 그것을 처리하기 위한 예약 행위
		
		

난제1	메모리 유닛그룹이 사용되지않는 프로세서에 갇히는 현상해결(메모리 낭비)
		Full 유닛은 확인하여 Detach 하는 것이 가능하다(이경우는 문제에서 제외 -> 사용된 메모리들이 모두 반납된 경우)
	Group 이 특정 프로세서 CACHE에 연결되어 계속 사용되지 않고 있다면 이것은 어떻게 활용하도록 할 것인가?
	마지막 사용시간을 기록 하는가?
	아니면 Expansion 요청시마다, 카운팅하며 n 회마다 다른 CACHE에 연결된 E_Busy 상태도 가져다 사용하도록 할 것인가?
		Allocator 의 Expansion counting 변수가 존재하기 때문에 이를 활용 가능하다
		유닛 크기에 따라 다르게 적용할 것인가?

난제2	속도저하 문제
	할당에서 최소 잠금 단계가 1에서 2로 증가
	반납에서 TMemoryUnitsGroup 접근으로 인하여 캐시미스 확률이 높은 문제(하지만 VirtualFree를 위해서는 어쩔수 없다)

체크	VirtualFree 로 인하여 TBB, LFH 에 비해 느려지는가?

체크	가능하다면 VirtualFree를 억제(재사용 가능성이 높다면)

기대	기존 방식의 n개단위로 pool의 공유자원에서
	가져오던 것에서, Group 단위로 가져오기 때문에 pool의 잠금 시간이 매우 줄어든다고 예측할 수 있다
	8스레드 프로파일링 결과에 의하면 할당시에 pool의 사용시간이 644/1021 = 63% 비중
	대부분 비중은 유닛을 n개 이동 시키는것으로 예상

기존 8스레드 프로파일링
	할당 프로세스 1021
		25	get CACHE L1
		319	CACHE L1 잠금
		20	get obj from CACHE L1
		2	get obj from CACHE L2, L3(this processor)
		99	get obj from pool
		2	get obj from CACHE L2, L3(other processor)
		488	pool 잠금
		57	pool allocator::expansion , get obj
		4	CACHE L1 잠금 해제
	해제 프로세스 134
		27	get CACHE L1
		65	CACHE L1 잠금
		2	CACHE L1에 반납
		나머지	상위 레벨에 CACHE L1 데이터 이동
		12	CACHE L1 잠금해제

새로운 구조로 인한 메모리 오버헤드
	128KB 할당시
	TDATA_VMEM_USED 1, TMemoryUnitsGroup 2 = total 192B
	비중은 ... 192B / 128KB = 0.146%




사전 에측 프로파일링
LOOP 1
12~256  1024MB
units 8012998
				T1			T8
기존				1.5198		0.8710
이중락 테스트			0.4481		0.1100 ~ 0.1400
캐시미스를 고려하여 약 2배 나빠진다고 추측한다
				0.8			0.3
여유분				0.7			0.5
